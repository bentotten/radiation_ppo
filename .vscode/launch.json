{
  // Use IntelliSense to learn about possible attributes.
  // Hover to view descriptions of existing attributes.
  // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
  "version": "0.2.0",
  "configurations": [
    {
      "name": "Python: Module",
      "type": "python",
      "request": "launch",
      "module": "unittest",
      "justMyCode": true,
      "args": ["unit_testing/unit_rad_ppo_agent.py"],
      "cwd": "${workspaceFolder}"
    },
    {
      "name": "RAD-A2C",
      "type": "python",
      //"python": "${command:python.interpreterPath}/../multi_ppo_torch", # Specify which interpreter
      "request": "launch",
      "program": "${workspaceFolder}/algos/ppo/main.py",
      "console": "integratedTerminal",
      "justMyCode": false,
      "args": [],
      "cwd": "${workspaceFolder}/algos/ppo"
    },
    {
      "name": "PLOT MULTI",
      "type": "python",
      //"python": "${command:python.interpreterPath}/../multi_ppo_torch", # Specify which interpreter
      "request": "launch",
      "program": "${workspaceFolder}/algos/multiagent/plot_results.py",
      "console": "integratedTerminal",
      "justMyCode": false,
      "args": ["--data_dir", "${workspaceFolder}/models", "--smooth", "10"],
      "cwd": "${workspaceFolder}/algos/multiagent"
    },
    {
      "name": "MULTI",
      "type": "python",
      "request": "launch",
      "program": "${workspaceFolder}/algos/multiagent/main.py",
      "console": "integratedTerminal",
      "justMyCode": false,
      "args": [
        "--agent-count",
        "1",
        "--steps-per-epoch",
        "1",
        "--steps-per-episode",
        "5",
        "--epochs",
        "3",
        "--render",
        "True",
        "--save-gif-freq",
        "200",
        "--obstruct",
        "0",
        "--seed",
        "2",
        "--enforce_grid_boundaries",
        "True",
        "--exp-name",
        "TEAM-RAD",
        "--net-type",
        "cnn"
      ],
      "cwd": "${workspaceFolder}/algos/multiagent"
    }
  ]
}

// General parameters for multi-agent
// help="Number of timesteps per epoch (before updating agent networks)"
// "--steps-per-epoch", "480",
// help="Number of total epochs to train the agent"
// "--steps-per-episode", "120",
// help="Number of total steps in an episode "
// "--epochs", "3000",
// help="Random seed control"
// "--seed", "2",
// help="Name of experiment for saving",
// "--exp-name", "test",
// help="Render Gif
// "--render", "False",
// help="Save frequency for gifs
// "--sav_gif_freq", "3",
// help="Save frequency for models
// "--sav_freq", "500",
// help="Number of agents"
// "--agent_count", "1",
// help="Environment name registered with Gym"
// "--env-name","gym_rad_search:RadSearchMulti-v1",

// Environment Parameters
// help="Dimensions of radiation source search area in cm, decreased by area_obs param. to ensure visilibity graph setup is valid. Length by height.",
// "--dims", "[2700.0, 2700.0]",
// help="Interval for each obstruction area in cm. This is how much to remove from bounds to make the "visible bounds"",
// "--area-obs", "[200.0, 500.0]",
// help="Number of obstructions present in each episode, options: -1 -> random sampling from [1,5], 0 -> no obstructions, [1-7] -> 1 to 7 obstructions",
// "--obstruct", "-1",
// help="Indicate whether or not agents can travel outside of the search area"
// "--enforce_grid_boundaries", "False",

// # Hyperparameters and PPO parameters
// help="Reward attribution for advantage estimator for PPO updates",
// "--gamma", "0.99"
// help="Entropy reward term scaling"
// "--alpha", "0.1",
// help="Batches to sample data during actor policy update (k_epochs)"
// "--minibatches", "1",

// # Parameters for Neural Networks
// help="Choose between recurrent neural network or MLP Actor-Critic (A2C), option: rnn, mlp",
// "--net-type", "rnn",
// help="Actor linear layer size (Policy Hidden Layer Size)"
// "--hid-pol", "32",
// help="Critic linear layer size (State-Value Hidden Layer Size)"
// "--hid-val", "32",
// help="PFGRU hidden state size (Localization Network)"
// "--hid-rec", "24",
// help="Actor-Critic GRU hidden state size (Embedding Layers)"
// "--hid-gru","24",
// help="Number of layers for Actor MLP (Policy Multi-layer Perceptron)"
// "--l-pol", "1",
// help="Number of layers for Critic MLP (State-Value Multi-layer Perceptron)"
// "--l-val","1",
